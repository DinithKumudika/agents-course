<CourseFloatingBanner chapter={2}
  classNames="absolute z-10 right-0 top-0"
  notebooks={[
    {label: "Google Colab", value: "https://colab.research.google.com/#fileId=https%3A//huggingface.co/agents-course/notebooks/blob/main/bonus-unit2/monitoring-and-evaluating-agents.ipynb"},
]} />

# Unit√© Bonus 2 : Observabilit√© et √âvaluation des Agents

<Tip>
Vous pouvez suivre le code dans <a href="https://colab.research.google.com/#fileId=https%3A//huggingface.co/agents-course/notebooks/blob/main/bonus-unit2/monitoring-and-evaluating-agents.ipynb" target="_blank">ce notebook</a> que vous pouvez ex√©cuter avec *Google Colab*.
</Tip>

Dans ce notebook, nous apprendrons comment **surveiller les √©tapes internes (traces) de notre agent IA** et **√©valuer ses performances** en utilisant des outils d'observabilit√© *open source*.

La capacit√© d'observer et d'√©valuer le comportement d'un agent est essentielle pour :
- D√©boguer les probl√®mes quand les t√¢ches √©chouent ou produisent des r√©sultats sous-optimaux
- Surveiller les co√ªts et performances en temps r√©el
- Am√©liorer la fiabilit√© et la s√©curit√© gr√¢ce √† des commentaires continus

## Pr√©requis de l'exercice üèóÔ∏è

Avant d'ex√©cuter ce notebook, assurez-vous d'avoir :

üî≤ üìö  **√âtudi√©** [Introduction aux Agents](https://huggingface.co/learn/agents-course/unit1/introduction)

üî≤ üìö  **√âtudi√©** [Le framework smolagents](https://huggingface.co/learn/agents-course/unit2/smolagents/introduction)

## √âtape 0 : Installer les biblioth√®ques requises

Nous aurons besoin de quelques biblioth√®ques qui nous permettent d'ex√©cuter, surveiller et √©valuer nos agents :

```python
%pip install langfuse 'smolagents[telemetry]' openinference-instrumentation-smolagents datasets 'smolagents[gradio]' gradio --upgrade
```

## √âtape 1 : Instrumenter votre Agent

Dans ce notebook, nous utiliserons [Langfuse](https://langfuse.com/) comme notre outil d'observabilit√©, mais vous pouvez utiliser **n'importe quel autre service compatible *OpenTelemetry***. Le code ci-dessous montre comment d√©finir les variables d'environnement pour *Langfuse* (ou tout endpoint *OTel*) et comment instrumenter votre *smolagent*.

**Note :** Si vous utilisez *LlamaIndex* ou *LangGraph*, vous pouvez trouver de la documentation sur leur instrumentation [ici](https://langfuse.com/docs/integrations/llama-index/workflows) et [ici](https://langfuse.com/docs/integrations/langchain/example-python-langgraph). 

D'abord, configurons les credentials *Langfuse* comme variables d'environnement. Obtenez vos cl√©s API *Langfuse* en vous inscrivant sur [Langfuse Cloud](https://cloud.langfuse.com) ou en [auto-h√©bergeant Langfuse](https://langfuse.com/self-hosting).

```python
import os
# Obtenez les cl√©s pour votre projet depuis la page des param√®tres du projet : https://cloud.langfuse.com
os.environ["LANGFUSE_PUBLIC_KEY"] = "pk-lf-..." 
os.environ["LANGFUSE_SECRET_KEY"] = "sk-lf-..." 
os.environ["LANGFUSE_HOST"] = "https://cloud.langfuse.com" # üá™üá∫ r√©gion EU
# os.environ["LANGFUSE_HOST"] = "https://us.cloud.langfuse.com" # üá∫üá∏ r√©gion US
```
Nous devons aussi configurer notre token *Hugging Face* pour les appels d'inf√©rence.

```python
# D√©finissez votre token Hugging Face et autres tokens/secrets comme variable d'environnement
os.environ["HF_TOKEN"] = "hf_..." 
```

Avec les variables d'environnement d√©finies, nous pouvons maintenant initialiser le client *Langfuse*. `get_client()` initialise le client *Langfuse* en utilisant les credentials fournis dans les variables d'environnement.

```python
from langfuse import get_client
 
langfuse = get_client()
 
# V√©rifier la connexion
if langfuse.auth_check():
    print("Le client Langfuse est authentifi√© et pr√™t !")
else:
    print("L'authentification a √©chou√©. Veuillez v√©rifier vos credentials et h√¥te.")
```

Ensuite, nous pouvons configurer le `SmolagentsInstrumentor()` pour instrumenter notre *smolagent* et envoyer des traces √† *Langfuse*.

```python
from openinference.instrumentation.smolagents import SmolagentsInstrumentor
 
SmolagentsInstrumentor().instrument()
```

## √âtape 2 : Tester votre instrumentation

Voici un simple *CodeAgent* de *smolagents* qui calcule `1+1`. Nous l'ex√©cutons pour confirmer que l'instrumentation fonctionne correctement. Si tout est configur√© correctement, vous verrez des logs/spans dans votre tableau de bord d'observabilit√©.

```python
from smolagents import InferenceClientModel, CodeAgent

# Cr√©er un agent simple pour tester l'instrumentation
agent = CodeAgent(
    tools=[],
    model=InferenceClientModel()
)

agent.run("1+1=")
```

V√©rifiez votre [Tableau de bord des traces Langfuse](https://cloud.langfuse.com) (ou votre outil d'observabilit√© choisi) pour confirmer que les spans et logs ont √©t√© enregistr√©s.

Exemple de capture d'√©cran de *Langfuse* :

![Example trace in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/first-example-trace.png)

_[Lien vers la trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1b94d6888258e0998329cdb72a371155?timestamp=2025-03-10T11%3A59%3A41.743Z)_

## √âtape 3 : Observer et √©valuer un agent plus complexe

Maintenant que vous avez confirm√© que votre instrumentation fonctionne, essayons une requ√™te plus complexe pour que nous puissions voir comment les m√©triques avanc√©es (utilisation de tokens, latence, co√ªts, etc.) sont suivies.

```python
from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)

search_tool = DuckDuckGoSearchTool()
agent = CodeAgent(tools=[search_tool], model=InferenceClientModel())

agent.run("Combien de Rubik's Cubes pourriez-vous faire tenir dans la Cath√©drale Notre-Dame ?")
```

### Structure de la trace

La plupart des outils d'observabilit√© enregistrent une **trace** qui contient des **spans**, qui repr√©sentent chaque √©tape de la logique de votre agent. Ici, la trace contient l'ex√©cution globale de l'agent et des sous-spans pour :
- Les appels d'outils (*DuckDuckGoSearchTool*)
- Les appels LLM (*InferenceClientModel*)

Vous pouvez les inspecter pour voir pr√©cis√©ment o√π le temps est pass√©, combien de tokens sont utilis√©s, etc. :

![Trace tree in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/trace-tree.png)

_[Lien vers la trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1ac33b89ffd5e75d4265b62900c348ed?timestamp=2025-03-07T13%3A45%3A09.149Z&display=preview)_

## √âvaluation en ligne

Dans la section pr√©c√©dente, nous avons appris la diff√©rence entre l'√©valuation en ligne et hors ligne. Maintenant, nous verrons comment surveiller votre agent en production et l'√©valuer en direct.

### M√©triques communes √† suivre en production

1. **Co√ªts** ‚Äî L'instrumentation *smolagents* capture l'utilisation de tokens, que vous pouvez transformer en co√ªts approximatifs en assignant un prix par token.
2. **Latence** ‚Äî Observer le temps qu'il faut pour compl√©ter chaque √©tape, ou l'ex√©cution enti√®re.
3. **Commentaires utilisateur** ‚Äî Les utilisateurs peuvent fournir des commentaires directs (pouces lev√©s/baiss√©s) pour aider √† raffiner ou corriger l'agent.
4. **LLM-as-a-Judge** ‚Äî Utiliser un LLM s√©par√© pour √©valuer la sortie de votre agent en temps quasi r√©el (par exemple, v√©rifier la toxicit√© ou l'exactitude).

Ci-dessous, nous montrons des exemples de ces m√©triques.

#### 1. Co√ªts

Ci-dessous se trouve une capture d'√©cran montrant l'utilisation pour les appels `Qwen2.5-Coder-32B-Instruct`. C'est utile pour voir les √©tapes co√ªteuses et optimiser votre agent. 

![Costs](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-costs.png)

_[Lien vers la trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1ac33b89ffd5e75d4265b62900c348ed?timestamp=2025-03-07T13%3A45%3A09.149Z&display=preview)_

#### 2. Latence

Nous pouvons aussi voir combien de temps il a fallu pour compl√©ter chaque √©tape. Dans l'exemple ci-dessous, toute la conversation a pris 32 secondes, que vous pouvez d√©composer par √©tape. Cela vous aide √† identifier les goulots d'√©tranglement et optimiser votre agent.

![Latency](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-latency.png)

_[Lien vers la trace](https://cloud.langfuse.com/project/cloramnkj0002jz088vzn1ja4/traces/1ac33b89ffd5e75d4265b62900c348ed?timestamp=2025-03-07T13%3A45%3A09.149Z&display=preview)_

#### 3. Attributs suppl√©mentaires

Vous pouvez aussi passer des attributs suppl√©mentaires √† vos spans. Ceux-ci peuvent inclure `user_id`, `tags`, `session_id`, et des m√©tadonn√©es personnalis√©es. Enrichir les traces avec ces d√©tails est important pour l'analyse, le d√©bogage et la surveillance du comportement de votre application √† travers diff√©rents utilisateurs ou sessions.

```python
from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)

search_tool = DuckDuckGoSearchTool()
agent = CodeAgent(
    tools=[search_tool],
    model=InferenceClientModel()
)

with langfuse.start_as_current_span(
    name="Smolagent-Trace",
    ) as span:
    
    # Ex√©cutez votre application ici
    response = agent.run("Quelle est la capitale de l'Allemagne ?")
 
    # Passez des attributs suppl√©mentaires au span
    span.update_trace(
        input="Quelle est la capitale de l'Allemagne ?",
        output=response,
        user_id="smolagent-user-123",
        session_id="smolagent-session-123456789",
        tags=["question-ville", "test-agents"],
        metadata={"email": "user@langfuse.com"},
        )
 
# Flusher les √©v√©nements dans les applications de courte dur√©e
langfuse.flush()
```

![Enhancing agent runs with additional metrics](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/smolagents-attributes.png)

#### 4. Commentaires utilisateur

Si votre agent est int√©gr√© dans une interface utilisateur, vous pouvez enregistrer des commentaires utilisateur directs (comme un pouce lev√©/baiss√© dans une interface de chat). Ci-dessous se trouve un exemple utilisant [Gradio](https://gradio.app/) pour int√©grer un chat avec un m√©canisme simple de commentaires.

Dans l'extrait de code ci-dessous, quand un utilisateur envoie un message de chat, nous capturons la trace dans *Langfuse*. Si l'utilisateur aime/n'aime pas la derni√®re r√©ponse, nous attachons un score √† la trace.

```python
import gradio as gr
from smolagents import (CodeAgent, InferenceClientModel)
from langfuse import get_client

langfuse = get_client()

model = InferenceClientModel()
agent = CodeAgent(tools=[], model=model, add_base_tools=True)

trace_id = None

def respond(prompt, history):
    with langfuse.start_as_current_span(
        name="Smolagent-Trace"):
        
        # Ex√©cutez votre application ici
        output = agent.run(prompt)

        global trace_id
        trace_id = langfuse.get_current_trace_id()

    history.append({"role": "assistant", "content": str(output)})
    return history

def handle_like(data: gr.LikeData):
    # Pour la d√©monstration, nous mappons les commentaires utilisateur √† 1 (j'aime) ou 0 (je n'aime pas)
    if data.liked:
        langfuse.create_score(
            value=1,
            name="user-feedback",
            trace_id=trace_id
        )
    else:
        langfuse.create_score(
            value=0,
            name="user-feedback",
            trace_id=trace_id
        )

with gr.Blocks() as demo:
    chatbot = gr.Chatbot(label="Chat", type="messages")
    prompt_box = gr.Textbox(placeholder="Tapez votre message...", label="Votre message")

    # Quand l'utilisateur appuie sur 'Entr√©e' sur le prompt, nous ex√©cutons 'respond'
    prompt_box.submit(
        fn=respond,
        inputs=[prompt_box, chatbot],
        outputs=chatbot
    )

    # Quand l'utilisateur clique sur un bouton 'j'aime' sur un message, nous ex√©cutons 'handle_like'
    chatbot.like(handle_like, None, None)

demo.launch()
```

Les commentaires utilisateur sont alors captur√©s dans votre outil d'observabilit√© :

![User feedback is being captured in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/user-feedback-gradio.png)

#### 5. LLM-as-a-Judge

*LLM-as-a-Judge* est une autre fa√ßon d'√©valuer automatiquement la sortie de votre agent. Vous pouvez configurer un appel LLM s√©par√© pour √©valuer l'exactitude, la toxicit√©, le style, ou tout autre crit√®re qui vous tient √† c≈ìur de la sortie.

**Flux de travail** :
1. Vous d√©finissez un **Template d'√©valuation**, par exemple, "V√©rifiez si le texte est toxique."
2. Chaque fois que votre agent g√©n√®re une sortie, vous passez cette sortie √† votre LLM "juge" avec le template.
3. Le LLM juge r√©pond avec une note ou un label que vous loggez dans votre outil d'observabilit√©.

Exemple de *Langfuse* :

![LLM-as-a-Judge Evaluation Template](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/evaluator-template.png)
![LLM-as-a-Judge Evaluator](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/evaluator.png)

```python
# Exemple : V√©rifier si la sortie de l'agent est toxique ou non.
from smolagents import (CodeAgent, DuckDuckGoSearchTool, InferenceClientModel)

search_tool = DuckDuckGoSearchTool()
agent = CodeAgent(tools=[search_tool], model=InferenceClientModel())

agent.run("Manger des carottes peut-il am√©liorer votre vision ?")
```

Vous pouvez voir que la r√©ponse de cet exemple est jug√©e comme "non toxique".

![LLM-as-a-Judge Evaluation Score](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/llm-as-a-judge-score.png)

#### 6. Aper√ßu des m√©triques d'observabilit√©

Toutes ces m√©triques peuvent √™tre visualis√©es ensemble dans des tableaux de bord. Cela vous permet de voir rapidement comment votre agent performe √† travers de nombreuses sessions et vous aide √† suivre les m√©triques de qualit√© au fil du temps.

![Observability metrics overview](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/langfuse-dashboard.png)

## √âvaluation hors ligne

L'√©valuation en ligne est essentielle pour les commentaires en direct, mais vous avez aussi besoin d'**√©valuation hors ligne**‚Äîdes v√©rifications syst√©matiques avant ou pendant le d√©veloppement. Cela aide √† maintenir la qualit√© et la fiabilit√© avant de d√©ployer les changements en production.

### √âvaluation de jeu de donn√©es

Dans l'√©valuation hors ligne, vous typiquement :
1. Avez un jeu de donn√©es de r√©f√©rence (avec des paires prompt et sortie attendue)
2. Ex√©cutez votre agent sur ce jeu de donn√©es
3. Comparez les sorties aux r√©sultats attendus ou utilisez un m√©canisme de notation suppl√©mentaire

Ci-dessous, nous d√©montrons cette approche avec le [jeu de donn√©es GSM8K](https://huggingface.co/datasets/gsm8k), qui contient des questions et solutions math√©matiques.

```python
import pandas as pd
from datasets import load_dataset

# R√©cup√©rer GSM8K de Hugging Face
dataset = load_dataset("openai/gsm8k", 'main', split='train')
df = pd.DataFrame(dataset)
print("Premi√®res lignes du jeu de donn√©es GSM8K :")
print(df.head())
```

Ensuite, nous cr√©ons une entit√© de jeu de donn√©es dans *Langfuse* pour suivre les ex√©cutions. Puis, nous ajoutons chaque √©l√©ment du jeu de donn√©es au syst√®me. (Si vous n'utilisez pas *Langfuse*, vous pourriez simplement stocker ceux-ci dans votre propre base de donn√©es ou fichier local pour l'analyse.)

```python
from langfuse import get_client
langfuse = get_client()

langfuse_dataset_name = "gsm8k_dataset_huggingface"

# Cr√©er un jeu de donn√©es dans Langfuse
langfuse.create_dataset(
    name=langfuse_dataset_name,
    description="Jeu de donn√©es de r√©f√©rence GSM8K t√©l√©charg√© depuis Huggingface",
    metadata={
        "date": "2025-03-10", 
        "type": "benchmark"
    }
)
```

```python
for idx, row in df.iterrows():
    langfuse.create_dataset_item(
        dataset_name=langfuse_dataset_name,
        input={"text": row["question"]},
        expected_output={"text": row["answer"]},
        metadata={"source_index": idx}
    )
    if idx >= 9: # T√©l√©charger seulement les 10 premiers √©l√©ments pour la d√©monstration
        break
```

![Dataset items in Langfuse](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/example-dataset.png)

#### Ex√©cuter l'agent sur le jeu de donn√©es

Nous d√©finissons une fonction helper `run_smolagent()` qui :
1. D√©marre un span *Langfuse*
2. Ex√©cute notre agent sur le prompt
3. Enregistre l'ID de trace dans *Langfuse*

Puis, nous bouclons sur chaque √©l√©ment du jeu de donn√©es, ex√©cutons l'agent, et lions la trace √† l'√©l√©ment du jeu de donn√©es. Nous pouvons aussi attacher un score d'√©valuation rapide si d√©sir√©.

```python
from opentelemetry.trace import format_trace_id
from smolagents import (CodeAgent, InferenceClientModel, LiteLLMModel)
from langfuse import get_client
 
langfuse = get_client()

# Exemple : utiliser InferenceClientModel ou LiteLLMModel pour acc√©der aux mod√®les openai, anthropic, gemini, etc. :
model = InferenceClientModel()

agent = CodeAgent(
    tools=[],
    model=model,
    add_base_tools=True
)

dataset_name = "gsm8k_dataset_huggingface"
current_run_name = "smolagent-notebook-run-01" # Identifie cette ex√©cution d'√©valuation sp√©cifique
 
# Supposons que 'run_smolagent' est votre fonction d'application instrument√©e
def run_smolagent(question):
    with langfuse.start_as_current_generation(name="qna-llm-call") as generation:
        # Simuler un appel LLM
        result = agent.run(question)
 
        # Mettre √† jour la trace avec l'entr√©e et la sortie
        generation.update_trace(
            input= question,
            output=result,
        )
 
        return result
 
dataset = langfuse.get_dataset(name=dataset_name) # R√©cup√©rer votre jeu de donn√©es pr√©-peupl√©
 
for item in dataset.items:
 
    # Utiliser le gestionnaire de contexte item.run()
    with item.run(
        run_name=current_run_name,
        run_metadata={"model_provider": "Hugging Face", "temperature_setting": 0.7},
        run_description="Ex√©cution d'√©valuation pour le jeu de donn√©es GSM8K"
    ) as root_span: # root_span est le span racine de la nouvelle trace pour cet √©l√©ment et ex√©cution.
        # Toutes les op√©rations langfuse suivantes dans ce bloc font partie de cette trace.
 
        # Appelez votre logique d'application
        generated_answer = run_smolagent(question=item.input["text"])
 
        print(item.input)
```

Vous pouvez r√©p√©ter ce processus avec diff√©rents :
- Mod√®les (*OpenAI GPT*, LLM local, etc.)
- Outils (recherche vs. pas de recherche)
- Prompts (diff√©rents messages syst√®me)

Puis les comparer c√¥te √† c√¥te dans votre outil d'observabilit√© :

![Dataset run overview](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/dataset_runs.png)
![Dataset run comparison](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/bonus-unit2/dataset-run-comparison.png)

## R√©flexions finales

Dans ce notebook, nous avons couvert comment :
1. **Configurer l'observabilit√©** en utilisant *smolagents* + exporteurs *OpenTelemetry*
2. **V√©rifier l'instrumentation** en ex√©cutant un agent simple
3. **Capturer des m√©triques d√©taill√©es** (co√ªt, latence, etc.) via des outils d'observabilit√©
4. **Collecter des commentaires utilisateur** via une interface *Gradio*
5. **Utiliser LLM-as-a-Judge** pour √©valuer automatiquement les sorties
6. **Effectuer une √©valuation hors ligne** avec un jeu de donn√©es de r√©f√©rence

ü§ó Bon codage !