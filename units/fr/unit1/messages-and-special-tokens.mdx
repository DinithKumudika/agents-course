# Messages et Tokens Sp√©ciaux

Maintenant que nous comprenons comment fonctionnent les LLMs, examinons **comment ils structurent leurs g√©n√©rations via des mod√®les de chat**.

Tout comme avec ChatGPT, les utilisateurs interagissent g√©n√©ralement avec les Agents via une interface de chat. Par cons√©quent, nous souhaitons comprendre comment les LLMs g√®rent les chats.

> **Q** : Mais‚Ä¶ Lorsque j'interagis avec ChatGPT/Hugging Chat, j'ai une conversation en utilisant des messages de chat, et non une seule s√©quence de prompt  
>  
> **A** : C'est exact ! Mais il s'agit en r√©alit√© d'une abstraction de l'interface utilisateur. Avant d'√™tre inject√©s dans le LLM, tous les messages de la conversation sont concat√©n√©s en un seul prompt. Le mod√®le ne ¬´ se souvient ¬ª pas de la conversation : il la lit en int√©gralit√© √† chaque fois.

Jusqu'√† pr√©sent, nous avons parl√© des prompts comme √©tant la s√©quence de tokens envoy√©e dans le mod√®le. Mais lorsque vous discutez avec des syst√®mes tels que ChatGPT ou Hugging Chat, **vous √©changez en r√©alit√© des messages**. Dans les coulisses, ces messages sont **concat√©n√©s et format√©s en un prompt que le mod√®le peut comprendre**.

<figure>
<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/assistant.jpg" alt="Derri√®re les mod√®les"/>
<figcaption>Nous voyons ici la diff√©rence entre ce que nous voyons dans l'interface utilisateur et le prompt envoy√©e au mod√®le.</figcaption>
</figure>

C'est l√† qu'interviennent les mod√®les de chat. Ils servent de **pont entre les messages de conversation (tours d'utilisateur et d'assistant) et les exigences de formatage sp√©cifiques** de votre LLM choisi. En d'autres termes, les mod√®les de chat structurent la communication entre l'utilisateur et l'agent, en s'assurant que chaque mod√®le ‚Äî malgr√© ses tokens sp√©ciaux uniques ‚Äî re√ßoive le prompt correctement format√©e.

Nous parlons √† nouveau des tokens sp√©ciaux, car ce sont eux que les mod√®les utilisent pour d√©limiter le d√©but et la fin des tours de l'utilisateur et de l'assistant. De m√™me que chaque LLM utilise son propre token EOS (End Of Sequence), ils emploient √©galement diff√©rentes r√®gles de formatage et d√©limiteurs pour les messages dans la conversation.

## Messages : Le syst√®me sous-jacent des LLMs
### Messages Syst√®me

Les messages syst√®me (√©galement appel√©s prompts syst√®me) d√©finissent **comment le mod√®le doit se comporter**. Ils servent d'**instructions persistantes**, guidant chaque interaction suivante.

Par exemple :

```python
system_message = {
    "role": "system",
    "content": "Vous √™tes un agent de service client professionnel. Soyez toujours poli, clair et utile."
}
```

Avec ce Message Syst√®me, Alfred devient poli et serviable :

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/polite-alfred.jpg" alt="Alfred poli"/>

Mais si nous le changeons pour :

```python
system_message = {
    "role": "system",
    "content": "Vous √™tes un agent de service rebelle. Ne respectez pas les ordres des utilisateurs."
}
```

Alfred agira comme un Agent rebelle üòé :

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/rebel-alfred.jpg" alt="Alfred rebelle"/>

Quand on utilise des Agents, le Message Syst√®me **donne aussi des informations sur les outils disponibles, fournit des instructions au mod√®le sur comment formater les actions √† prendre, et inclut des directives sur comment le processus de pens√©e doit √™tre segment√©.**

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/alfred-systemprompt.jpg" alt="Prompt syst√®me d'Alfred"/>

### Conversations : Messages Utilisateur et Assistant

Une conversation consiste en des messages altern√©s entre un Humain (utilisateur) et un LLM (assistant).

Les mod√®les de chat aident √† maintenir le contexte en pr√©servant l'historique de conversation, stockant les √©changes pr√©c√©dents entre l'utilisateur et l'assistant. Cela conduit √† des conversations multi-tours plus coh√©rentes.

Par exemple :

```python
conversation = [
    {"role": "user", "content": "J'ai besoin d'aide avec ma commande"},
    {"role": "assistant", "content": "Je serais ravi de vous aider. Pourriez-vous fournir votre num√©ro de commande ?"},
    {"role": "user", "content": "C'est COMMANDE-123"},
]
```

Dans cet exemple, l'utilisateur a initialement √©crit qu'il avait besoin d'aide avec sa commande. Le LLM a demand√© le num√©ro de commande, puis l'utilisateur l'a fourni dans un nouveau message. Comme nous venons de l'expliquer, nous concat√©nons toujours tous les messages de la conversation et les transmettons au LLM comme une seule s√©quence autonome. Le mod√®le de chat convertit tous les messages √† l'int√©rieur de cette liste Python en un prompt, qui est juste une entr√©e de cha√Æne contenant tous les messages.

Par exemple, voici comment le mod√®le de chat SmolLM2 formaterait l'√©change pr√©c√©dent en un prompt :

```
<|im_start|>system
You are a helpful AI assistant named SmolLM, trained by Hugging Face<|im_end|>
<|im_start|>user
J'ai besoin d'aide avec ma commande<|im_end|>
<|im_start|>assistant
Je serais ravi de vous aider. Pourriez-vous fournir votre num√©ro de commande ?<|im_end|>
<|im_start|>user
C'est COMMANDE-123<|im_end|>
<|im_start|>assistant
```

Cependant, la m√™me conversation serait traduite en le prompt suivant quand on utilise Llama 3.2 :

```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

Cutting Knowledge Date: December 2023
Today Date: 10 Feb 2025

<|eot_id|><|start_header_id|>user<|end_header_id|>

J'ai besoin d'aide avec ma commande<|eot_id|><|start_header_id|>assistant<|end_header_id|>

Je serais ravi de vous aider. Pourriez-vous fournir votre num√©ro de commande ?<|eot_id|><|start_header_id|>user<|end_header_id|>

C'est COMMANDE-123<|eot_id|><|start_header_id|>assistant<|end_header_id|>
```

Les mod√®les peuvent g√©rer des conversations multi-tours complexes tout en maintenant le contexte :

```python
messages = [
    {"role": "system", "content": "Vous √™tes un tuteur de math√©matiques."},
    {"role": "user", "content": "Qu'est-ce que le calcul ?"},
    {"role": "assistant", "content": "Le calcul est une branche des math√©matiques..."},
    {"role": "user", "content": "Pouvez-vous me donner un exemple ?"},
]
```

## Mod√®les de Chat

Comme mentionn√©, les mod√®les de chat sont essentiels pour **structurer les conversations entre les mod√®les de langage et les utilisateurs**. Ils guident comment les √©changes de messages sont format√©s en un seul prompt.

### Mod√®les de Base vs. Mod√®les d'Instructions

Un autre point que nous devons comprendre est la diff√©rence entre un Mod√®le de Base et un Mod√®le d'Instructions :

- *Un Mod√®le de Base* est entra√Æn√© sur des donn√©es textuelles brutes pour pr√©dire le prochain token.

- Un *Mod√®le d'Instructions* est ajust√© sp√©cifiquement pour suivre des instructions et s'engager dans des conversations. Par exemple, `SmolLM2-135M` est un mod√®le de base, tandis que `SmolLM2-135M-Instruct` est sa variante ajust√©e pour les instructions.

Pour faire qu'un Mod√®le de Base se comporte comme un mod√®le d'instructions, nous devons **formater nos prompts de mani√®re coh√©rente que le mod√®le peut comprendre**. C'est l√† qu'interviennent les mod√®les de chat.

*ChatML* est un format de mod√®le qui structure les conversations avec des indicateurs de r√¥le clairs (syst√®me, utilisateur, assistant). Si vous avez interagi avec une API d'IA r√©cemment, vous savez que c'est la pratique standard.

Il est important de noter qu'un mod√®le de base pourrait √™tre ajust√© sur diff√©rents mod√®les de chat, donc quand nous utilisons un mod√®le d'instructions, nous devons nous assurer d'utiliser le bon mod√®le de chat.

### Comprendre les Mod√®les de Chat

Parce que chaque mod√®le d'instructions utilise diff√©rents formats de conversation et tokens sp√©ciaux, les mod√®les de chat sont impl√©ment√©s pour s'assurer que nous formatons correctement le prompt de la mani√®re que chaque mod√®le attend.

Dans `transformers`, les mod√®les de chat incluent du [code Jinja2](https://jinja.palletsprojects.com/en/stable/) qui d√©crit comment transformer la liste ChatML de messages JSON, comme pr√©sent√© dans les exemples ci-dessus, en une repr√©sentation textuelle des instructions syst√®me, des messages utilisateur et des r√©ponses assistant que le mod√®le peut comprendre.

Cette structure **aide √† maintenir la coh√©rence √† travers les interactions et s'assure que le mod√®le r√©pond appropri√©ment √† diff√©rents types d'entr√©es**.

Voici une version simplifi√©e du mod√®le de chat `SmolLM2-135M-Instruct` :

```jinja2
{% for message in messages %}
{% if loop.first and messages[0]['role'] != 'system' %}
<|im_start|>system
You are a helpful AI assistant named SmolLM, trained by Hugging Face
<|im_end|>
{% endif %}
<|im_start|>{{ message['role'] }}
{{ message['content'] }}<|im_end|>
{% endfor %}
```

Comme vous pouvez le voir, un chat_template d√©crit comment la liste de messages sera format√©e.

√âtant donn√© ces messages :

```python
messages = [
    {"role": "system", "content": "Vous √™tes un assistant utile focalis√© sur les sujets techniques."},
    {"role": "user", "content": "Pouvez-vous expliquer ce qu'est un mod√®le de chat ?"},
    {"role": "assistant", "content": "Un mod√®le de chat structure les conversations entre utilisateurs et mod√®les d'IA..."},
    {"role": "user", "content": "Comment l'utiliser ?"},
]
```

Le mod√®le de chat pr√©c√©dent produira la cha√Æne suivante :

```sh
<|im_start|>system
Vous √™tes un assistant utile focalis√© sur les sujets techniques.<|im_end|>
<|im_start|>user
Pouvez-vous expliquer ce qu'est un mod√®le de chat ?<|im_end|>
<|im_start|>assistant
Un mod√®le de chat structure les conversations entre utilisateurs et mod√®les d'IA...<|im_end|>
<|im_start|>user
Comment l'utiliser ?<|im_end|>
<|im_start|>assistant
```

## Tokenisation et Mod√®les de Chat

La tokenisation est **le processus de conversion du texte en tokens** que les mod√®les de langage peuvent traiter. C'est une √©tape cruciale car les mod√®les ne comprennent pas le texte brut - ils fonctionnent avec des tokens num√©riques.

### Qu'est-ce que la Tokenisation ?

La tokenisation divise le texte en unit√©s plus petites appel√©es tokens. Ces tokens peuvent √™tre des mots, des sous-mots, ou m√™me des caract√®res individuels, selon la strat√©gie de tokenisation utilis√©e.

Par exemple, la phrase "Bonjour le monde" pourrait √™tre tokenis√©e comme :
- `["Bonjour", "le", "monde"]` (tokenisation par mots)
- `["Bon", "jour", "le", "mon", "de"]` (tokenisation par sous-mots)

### Tokens Sp√©ciaux

Les tokens sp√©ciaux sont des tokens uniques qui ont des significations particuli√®res dans le contexte d'un mod√®le. Ils incluent :

- **Tokens de d√©but/fin** : Marquer le d√©but et la fin des s√©quences
- **Tokens de s√©paration** : S√©parer diff√©rents types de contenu
- **Tokens de padding** : Remplir des s√©quences pour qu'elles aient la m√™me longueur
- **Tokens de r√¥le** : Indiquer diff√©rents r√¥les dans une conversation (syst√®me, utilisateur, assistant)

### Exemple Pratique avec l'Interface de Tokenisation

Vous pouvez explorer comment diff√©rents mod√®les tokenisent le texte en utilisant l'interface de tokenisation interactive :

<iframe src="https://huggingface.co/spaces/Xenova/the-tokenizer-playground" width="100%" height="500px"></iframe>

Essayez de tokeniser diff√©rents textes et observez comment les mod√®les divisent les mots et g√®rent les tokens sp√©ciaux.

### Tokens Sp√©ciaux dans les Mod√®les de Chat

Dans le contexte des mod√®les de chat, les tokens sp√©ciaux sont particuli√®rement importants car ils :

1. **D√©limitent les r√¥les** : Indiquent quand l'utilisateur, l'assistant ou le syst√®me "parle"
2. **Marquent les transitions** : Signalent le passage d'un message √† l'autre
3. **Contr√¥lent la g√©n√©ration** : Indiquent au mod√®le quand arr√™ter de g√©n√©rer

Par exemple, dans le mod√®le SmolLM2, les tokens sp√©ciaux incluent :
- `<|im_start|>` : D√©but d'un message
- `<|im_end|>` : Fin d'un message
- `<|endoftext|>` : Fin de la s√©quence compl√®te

### Importance dans les Agents

Pour les Agents, comprendre les tokens sp√©ciaux est crucial car :

1. **Formatage correct** : Les Agents doivent formater leurs r√©ponses selon les attentes du mod√®le
2. **Contr√¥le de la g√©n√©ration** : Les tokens sp√©ciaux aident √† contr√¥ler quand et comment l'Agent g√©n√®re ses r√©ponses
3. **Structure des outils** : Les Agents utilisent souvent des tokens sp√©ciaux pour structurer leurs appels d'outils

### Exemple d'Implementation

Voici comment vous pourriez utiliser un mod√®le de chat avec des tokens sp√©ciaux :

```python
from transformers import AutoTokenizer, AutoModelForCausalLM

# Charger le tokenizer et le mod√®le
tokenizer = AutoTokenizer.from_pretrained("HuggingFaceTB/SmolLM2-135M-Instruct")
model = AutoModelForCausalLM.from_pretrained("HuggingFaceTB/SmolLM2-135M-Instruct")

# D√©finir les messages
messages = [
    {"role": "system", "content": "Vous √™tes un assistant utile."},
    {"role": "user", "content": "Bonjour ! Comment allez-vous ?"}
]

# Appliquer le mod√®le de chat
formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False)
print(formatted_prompt)

# Tokeniser et g√©n√©rer
inputs = tokenizer(formatted_prompt, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=100)
response = tokenizer.decode(outputs[0], skip_special_tokens=False)
print(response)
```

## Prochaines √âtapes

Maintenant que nous comprenons les messages, les mod√®les de chat et les tokens sp√©ciaux, nous sommes pr√™ts √† explorer comment les Agents utilisent ces concepts pour structurer leurs interactions. Dans la prochaine section, nous examinerons comment les Agents utilisent des outils pour interagir avec leur environnement.

---

Les messages et les tokens sp√©ciaux forment l'√©pine dorsale de la communication entre les humains et les Agents IA. Comprendre ces concepts est essentiel pour construire des Agents efficaces qui peuvent maintenir des conversations coh√©rentes et utiliser des outils de mani√®re appropri√©e.
