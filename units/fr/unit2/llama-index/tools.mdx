# Utiliser les *Tools* dans LlamaIndex

**Définir un ensemble clair de *Tools* est crucial pour la performance.** Comme nous l'avons discuté dans l'[unité 1](../../unit1/tools), des interfaces de *tools* claires sont plus faciles à utiliser pour les *LLMs*.
Tout comme une interface API logicielle pour les ingénieurs humains, ils peuvent tirer plus de profit de l'outil s'il est facile de comprendre comment il fonctionne.

Il y a **quatre types principaux de *tools* dans LlamaIndex** :

![Tools](https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit2/llama-index/tools.png)

1. `FunctionTool` : Convertit n'importe quelle fonction Python en un *tool* qu'un agent peut utiliser. Il comprend automatiquement comment la fonction fonctionne.
2. `QueryEngineTool` : Un *tool* qui permet aux agents d'utiliser des *query engines*. Puisque les agents sont construits sur des *query engines*, ils peuvent également utiliser d'autres agents comme *tools*.
3. `Toolspecs` : Ensembles de *tools* créés par la communauté, qui incluent souvent des *tools* pour des services spécifiques comme Gmail.
4. `Utility Tools` : *Tools* spéciaux qui aident à gérer de grandes quantités de données d'autres *tools*.

Nous passerons en revue chacun d'eux plus en détail ci-dessous.

## Créer un *FunctionTool*

<Tip>
Vous pouvez suivre le code dans <a href="https://huggingface.co/agents-course/notebooks/blob/main/unit2/llama-index/tools.ipynb" target="_blank">ce *notebook*</a> que vous pouvez exécuter avec Google Colab.
</Tip>

Un *FunctionTool* fournit un moyen simple d'envelopper n'importe quelle fonction Python et de la rendre disponible à un agent.
Vous pouvez passer soit une fonction synchrone soit asynchrone au *tool*, avec des paramètres optionnels `name` et `description`.
Le nom et la description sont particulièrement importants car ils aident l'agent à comprendre quand et comment utiliser le *tool* efficacement.
Regardons comment créer un *FunctionTool* ci-dessous puis l'appeler.

```python
from llama_index.core.tools import FunctionTool

def get_weather(location: str) -> str:
    """Useful for getting the weather for a given location."""
    print(f"Getting weather for {location}")
    return f"The weather in {location} is sunny"

tool = FunctionTool.from_defaults(
    get_weather,
    name="my_weather_tool",
    description="Useful for getting the weather for a given location.",
)
tool.call("New York")
```

<Tip>Lors de l'utilisation d'un agent ou d'un *LLM* avec l'appel de fonctions, le *tool* sélectionné (et les arguments écrits pour ce *tool*) dépendent fortement du nom du *tool* et de la description du but et des arguments du *tool*. Apprenez-en plus sur l'appel de fonctions dans le <a href="https://docs.llamaindex.ai/en/stable/examples/workflow/function_calling_agent/">*Guide* de l'appel de fonctions</a>.</Tip>

## Créer un *QueryEngineTool*

Le `QueryEngine` que nous avons défini dans l'unité précédente peut être facilement transformé en un *tool* en utilisant la classe `QueryEngineTool`.
Voyons comment créer un `QueryEngineTool` à partir d'un `QueryEngine` dans l'exemple ci-dessous.

```python
from llama_index.core import VectorStoreIndex
from llama_index.core.tools import QueryEngineTool
from llama_index.llms.huggingface_api import HuggingFaceInferenceAPI
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.vector_stores.chroma import ChromaVectorStore

embed_model = HuggingFaceEmbedding("BAAI/bge-small-en-v1.5")

db = chromadb.PersistentClient(path="./alfred_chroma_db")
chroma_collection = db.get_or_create_collection("alfred")
vector_store = ChromaVectorStore(chroma_collection=chroma_collection)

index = VectorStoreIndex.from_vector_store(vector_store, embed_model=embed_model)

llm = HuggingFaceInferenceAPI(model_name="Qwen/Qwen2.5-Coder-32B-Instruct")
query_engine = index.as_query_engine(llm=llm)
tool = QueryEngineTool.from_defaults(query_engine, name="some useful name", description="some useful description")
```

## Créer des *Toolspecs*

Pensez aux `ToolSpecs` comme des collections de *tools* qui fonctionnent ensemble harmonieusement - comme une *toolkit* professionnelle bien organisée.
Tout comme la *toolkit* d'un mécanicien contient des *tools* complémentaires qui fonctionnent ensemble pour les réparations de véhicules, un `ToolSpec` combine des *tools* apparentés pour des objectifs spécifiques.
Par exemple, le `ToolSpec` d'un agent comptable pourrait intégrer élégamment des capacités de tableur, des fonctionnalités email et des *tools* de calcul pour gérer les tâches financières avec précision et efficacité.

<details>
<summary>Installer le *Toolspec* Google</summary>
Comme introduit dans la <a href="./llama-hub">section sur le *LlamaHub*</a>, nous pouvons installer le *toolspec* Google avec la commande suivante :

```python
pip install llama-index-tools-google
```
</details>

Et maintenant nous pouvons charger le *toolspec* et le convertir en une liste de *tools*.

```python
from llama_index.tools.google import GmailToolSpec

tool_spec = GmailToolSpec()
tool_spec_list = tool_spec.to_tool_list()
```

Pour obtenir une vue plus détaillée des *tools*, nous pouvons examiner les `metadata` de chaque *tool*.

```python
[(tool.metadata.name, tool.metadata.description) for tool in tool_spec_list]
```

### *Model Context Protocol* (*MCP*) dans LlamaIndex

LlamaIndex permet également d'utiliser les *tools* *MCP* grâce à un [*ToolSpec* sur le *LlamaHub*](https://llamahub.ai/l/tools/llama-index-tools-mcp?from=).
Vous pouvez simplement lancer un serveur *MCP* et commencer à l'utiliser grâce à l'implémentation suivante.

Si vous voulez approfondir *MCP*, vous pouvez consulter notre [cours *MCP* gratuit](https://huggingface.co/learn/mcp-course/). 

<details>
<summary>Installer le *Toolspec* *MCP*</summary>
Comme introduit dans la <a href="./llama-hub">section sur le *LlamaHub*</a>, nous pouvons installer le *toolspec* *MCP* avec la commande suivante :

```python
pip install llama-index-tools-mcp
```
</details>

```python
from llama_index.tools.mcp import BasicMCPClient, McpToolSpec

# We consider there is a mcp server running on 127.0.0.1:8000, or you can use the mcp client to connect to your own mcp server.
mcp_client = BasicMCPClient("http://127.0.0.1:8000/sse")
mcp_tool = McpToolSpec(client=mcp_client)

# get the agent
agent = await get_agent(mcp_tool)

# create the agent context
agent_context = Context(agent)
```

## *Utility Tools*

Souvent, interroger directement une API **peut retourner une quantité excessive de données**, dont certaines peuvent être non pertinentes, déborder la fenêtre de contexte du *LLM*, ou augmenter inutilement le nombre de *tokens* que vous utilisez.
Passons en revue nos deux *utility tools* principaux ci-dessous.

1. `OnDemandToolLoader` : Ce *tool* transforme n'importe quel *data loader* LlamaIndex existant (classe *BaseReader*) en un *tool* qu'un agent peut utiliser. Le *tool* peut être appelé avec tous les paramètres nécessaires pour déclencher `load_data` du *data loader*, ainsi qu'une chaîne de requête en langage naturel. Pendant l'exécution, nous chargeons d'abord les données du *data loader*, les indexons (par exemple avec un *vector store*), puis les interrogeons 'à la demande'. Ces trois étapes se produisent dans un seul appel de *tool*.
2. `LoadAndSearchToolSpec` : Le *LoadAndSearchToolSpec* prend n'importe quel *Tool* existant en entrée. En tant que *tool spec*, il implémente `to_tool_list`, et quand cette fonction est appelée, deux *tools* sont retournés : un *tool* de chargement puis un *tool* de recherche. L'exécution du *Tool* de chargement appellerait le *Tool* sous-jacent, puis indexerait la sortie (par défaut avec un *vector index*). L'exécution du *Tool* de recherche prendrait une chaîne de requête en entrée et appellerait l'index sous-jacent.

<Tip>Vous pouvez trouver des *toolspecs* et des *utility tools* sur le <a href="https://llamahub.ai/">*LlamaHub*</a></Tip>

Maintenant que nous comprenons les bases des agents et des *tools* dans LlamaIndex, voyons comment nous pouvons **utiliser LlamaIndex pour créer des *workflows* configurables et gérables !**