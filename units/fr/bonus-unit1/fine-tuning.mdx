# Ajustons finement votre mod√®le pour l'appel de fonctions

Nous sommes maintenant pr√™ts √† ajuster finement notre premier mod√®le pour l'appel de fonctions üî•.

## Comment entra√Ænons-nous notre mod√®le pour l'appel de fonctions ?

> R√©ponse : Nous avons besoin de **donn√©es**

Un processus d'entra√Ænement de mod√®le peut √™tre divis√© en 3 √©tapes :

1. **Le mod√®le est pr√©-entra√Æn√© sur une grande quantit√© de donn√©es**. Le r√©sultat de cette √©tape est un **mod√®le pr√©-entra√Æn√©**. Par exemple, [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b). C'est un mod√®le de base et il sait seulement comment **pr√©dire le prochain token sans fortes capacit√©s de suivi d'instructions**.

2. Pour √™tre utile dans un contexte de chat, le mod√®le doit ensuite √™tre **ajust√© finement** pour suivre des instructions. √Ä cette √©tape, il peut √™tre entra√Æn√© par les cr√©ateurs du mod√®le, la communaut√© open-source, vous, ou n'importe qui. Par exemple, [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) est un mod√®le ajust√© pour les instructions par l'√©quipe Google derri√®re le projet Gemma.

3. Le mod√®le peut ensuite √™tre **align√©** selon les pr√©f√©rences du cr√©ateur. Par exemple, un mod√®le de chat de service client qui ne doit jamais √™tre impoli avec les clients.

Habituellement, un produit complet comme *Gemini* ou *Mistral* **passera par les 3 √©tapes**, alors que les mod√®les que vous pouvez trouver sur *Hugging Face* ont compl√©t√© une ou plusieurs √©tapes de cet entra√Ænement.

Dans ce tutoriel, nous construirons un mod√®le d'appel de fonctions bas√© sur [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it). Nous choisissons le mod√®le ajust√© finement [google/gemma-2-2b-it](https://huggingface.co/google/gemma-2-2b-it) au lieu du mod√®le de base [google/gemma-2-2b](https://huggingface.co/google/gemma-2-2b) parce que le mod√®le ajust√© finement a √©t√© am√©lior√© pour notre cas d'usage.

Partir du mod√®le pr√©-entra√Æn√© **n√©cessiterait plus d'entra√Ænement pour apprendre le suivi d'instructions, le chat ET l'appel de fonctions**.

En partant du mod√®le ajust√© pour les instructions, **nous minimisons la quantit√© d'informations que notre mod√®le doit apprendre**.

## LoRA (*Low-Rank Adaptation of Large Language Models*)

LoRA est une technique d'entra√Ænement populaire et l√©g√®re qui **r√©duit significativement le nombre de param√®tres entra√Ænables**.

Elle fonctionne en **ins√©rant un plus petit nombre de nouveaux poids comme adaptateur dans le mod√®le √† entra√Æner**. Cela rend l'entra√Ænement avec LoRA beaucoup plus rapide, √©conome en m√©moire, et produit des poids de mod√®le plus petits (quelques centaines de MB), qui sont plus faciles √† stocker et partager.

<img src="https://huggingface.co/datasets/agents-course/course-images/resolve/main/en/unit1/blog_multi-lora-serving_LoRA.gif" alt="LoRA inference" width="50%"/>

LoRA fonctionne en ajoutant des paires de matrices de d√©composition de rang aux couches *Transformer*, se concentrant typiquement sur les couches lin√©aires. Durant l'entra√Ænement, nous "g√®lerons" le reste du mod√®le et ne mettrons √† jour que les poids de ces adaptateurs nouvellement ajout√©s.

Ce faisant, le nombre de **param√®tres** que nous devons entra√Æner chute consid√©rablement car nous devons seulement mettre √† jour les poids de l'adaptateur.

Durant l'inf√©rence, l'entr√©e est pass√©e dans l'adaptateur et le mod√®le de base, ou ces poids d'adaptateur peuvent √™tre fusionn√©s avec le mod√®le de base, ne r√©sultant en aucune surcharge de latence suppl√©mentaire.

LoRA est particuli√®rement utile pour adapter des mod√®les de langage **larges** √† des t√¢ches ou domaines sp√©cifiques tout en gardant les exigences de ressources g√©rables. Cela aide √† r√©duire la m√©moire **requise** pour entra√Æner un mod√®le.

Si vous voulez en savoir plus sur comment LoRA fonctionne, vous devriez consulter ce [tutoriel](https://huggingface.co/learn/nlp-course/chapter11/4?fw=pt).

## Ajustement fin d'un mod√®le pour l'appel de fonctions

Vous pouvez acc√©der au notebook du tutoriel üëâ [ici](https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb).

Ensuite, cliquez sur [![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/#fileId=https://huggingface.co/agents-course/notebooks/blob/main/bonus-unit1/bonus-unit1.ipynb) pour pouvoir l'ex√©cuter dans un *Notebook Colab*.